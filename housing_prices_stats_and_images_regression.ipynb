{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "housing_prices_stats_and_images_regression.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "LulBXVZiU9Hq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "37e6cee0-e30a-4a9d-dd25-f8db410f7314"
      },
      "source": [
        "!git clone https://github.com/emanhamed/Houses-dataset"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Houses-dataset'...\n",
            "remote: Enumerating objects: 5, done.\u001b[K\n",
            "remote: Counting objects: 100% (5/5), done.\u001b[K\n",
            "remote: Compressing objects: 100% (5/5), done.\u001b[K\n",
            "remote: Total 2166 (delta 0), reused 0 (delta 0), pack-reused 2161\u001b[K\n",
            "Receiving objects: 100% (2166/2166), 176.31 MiB | 21.93 MiB/s, done.\n",
            "Resolving deltas: 100% (18/18), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VHLxQTsVmm5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "import cv2\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BM8hNvO5Vq8s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e1632555-ea5e-4a9c-e86f-77cab7dfa921"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.layers.convolutional import MaxPooling2D\n",
        "from keras.layers.core import Activation\n",
        "from keras.layers.core import Dropout\n",
        "from keras.layers.core import Dense\n",
        "from keras.layers import concatenate\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Input\n",
        "from keras.models import Model"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snzAB47XVsuB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_house_attributes(inputPath):\n",
        "    cols = [\"bedrooms\", \"bathrooms\", \"area\", \"zipcode\", \"price\"]\n",
        "    df = pd.read_csv(inputPath, sep=' ', header=None, names=cols)\n",
        "    \n",
        "    zipcodes = df['zipcode'].value_counts().keys().tolist()\n",
        "    counts   = df['zipcode'].value_counts().tolist()\n",
        "    #df.head()\n",
        "    \n",
        "    for (zipcode, count) in zip(zipcodes, counts):\n",
        "      \n",
        "        if count < 25:\n",
        "            idxs = df[df['zipcode'] == zipcode].index\n",
        "            df.drop(idxs, inplace=True)\n",
        "            \n",
        "    return df\n",
        "  \n",
        "def process_house_attributes(df, train, test):\n",
        "    continuous = ['bedrooms', 'bathrooms', 'area']\n",
        "    \n",
        "    cs               = MinMaxScaler()\n",
        "    train_continuous = cs.fit_transform(train[continuous])\n",
        "    test_continuous  = cs.transform(test[continuous])\n",
        "    \n",
        "    zip_binarizer     = LabelBinarizer().fit(df['zipcode'])\n",
        "    train_categorical = zip_binarizer.transform(train['zipcode'])\n",
        "    test_categorical  = zip_binarizer.transform(test['zipcode'])\n",
        "    \n",
        "    train_X = np.hstack([train_categorical, train_continuous])\n",
        "    test_X  = np.hstack([test_categorical, test_continuous])\n",
        "    \n",
        "    return (train_X, test_X)\n",
        "  \n",
        "def load_house_images(df, inputPath):\n",
        "    images = []\n",
        "    \n",
        "    for i in df.index.values:\n",
        "        base_path   = os.path.sep.join([inputPath, \"{}_*\".format(i + 1)])\n",
        "        house_paths = sorted(list(glob.glob(base_path)))\n",
        "        \n",
        "        input_images = []\n",
        "        output_image = np.zeros((64, 64, 3), dtype=\"uint8\")\n",
        "        \n",
        "        for house_path in house_paths:\n",
        "            image = cv2.imread(house_path)\n",
        "            image = cv2.resize(image, (32, 32))\n",
        "            input_images.append(image)\n",
        "            \n",
        "        output_image[0:32, 0:32]   = input_images[0]\n",
        "        output_image[0:32, 32:64]  = input_images[1]\n",
        "        output_image[32:64, 32:64] = input_images[2]\n",
        "        output_image[32:64, 0:32]  = input_images[3]\n",
        "        \n",
        "        images.append(output_image)\n",
        "        \n",
        "    return np.array(images)\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2iO-XIJpwWB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_mlp(dim, regress=False):\n",
        "    # define our MLP network\n",
        "\t  model = Sequential()\n",
        "\t  model.add(Dense(8, input_dim=dim, activation=\"relu\"))\n",
        "\t  model.add(Dense(4, activation=\"relu\"))\n",
        " \n",
        "\t  # check to see if the regression node should be added\n",
        "\t  if regress:\n",
        "\t\t    model.add(Dense(1, activation=\"linear\"))\n",
        " \n",
        "\t  # return our model\n",
        "\t  return model\n",
        "\n",
        "def create_cnn(width, height, depth, filters=(16, 32, 64), regress=False):\n",
        "  \n",
        "    input_shape = (height, width, depth)\n",
        "    chan_dim = -1\n",
        "    \n",
        "    inputs = Input(shape=input_shape)\n",
        "    \n",
        "    for (i, f) in enumerate(filters):\n",
        "        if i == 0:\n",
        "            x = inputs\n",
        "            \n",
        "        x = Conv2D(f, (3, 3), padding=\"same\")(x)\n",
        "        x = Activation(\"relu\")(x)\n",
        "        x = BatchNormalization(axis=chan_dim)(x)\n",
        "        x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "        \n",
        "    x = Flatten()(x)\n",
        "    x = Dense(16)(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "    x = BatchNormalization(axis=chan_dim)(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    \n",
        "    x = Dense(4)(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "    \n",
        "    if regress:\n",
        "        x = Dense(1, activation=\"linear\")(x)\n",
        "        \n",
        "    model = Model(inputs, x)\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rdl-aHKfqIZI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cwd = os.getcwd()\n",
        "house_jpg_dir = os.path.join(cwd, 'Houses-dataset', 'Houses Dataset')\n",
        "houses_info   = os.path.join(house_jpg_dir, 'HousesInfo.txt')\n",
        "\n",
        "df = load_house_attributes(houses_info)\n",
        "\n",
        "images = load_house_images(df, house_jpg_dir)\n",
        "\n",
        "# Scaling pixel intensities to the range [0, 1].\n",
        "images = images / 255.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syqgh4KyrE6N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "split = train_test_split(df, images, test_size=0.25, random_state=42)\n",
        "(trainAttrX, testAttrX, trainImagesX, testImagesX) = split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlSsvKHZrQmP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "maxPrice = trainAttrX[\"price\"].max()\n",
        "trainY = trainAttrX[\"price\"] / maxPrice\n",
        "testY = testAttrX[\"price\"] / maxPrice"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_95uEsdrTgA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(trainAttrX, testAttrX) = process_house_attributes(df, trainAttrX, testAttrX)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nvpkw76rW9k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "outputId": "b41ef4bb-2231-4cf3-944d-9effc9a560f2"
      },
      "source": [
        "mlp = create_mlp(trainAttrX.shape[1], regress=False)\n",
        "cnn = create_cnn(64, 64, 3, regress=False)\n",
        "\n",
        "combinedInput = concatenate([mlp.output, cnn.output])\n",
        " \n",
        "# our final FC layer head will have two dense layers, the final one\n",
        "# being our regression head\n",
        "x = Dense(4, activation=\"relu\")(combinedInput)\n",
        "x = Dense(1, activation=\"linear\")(x)\n",
        " \n",
        "# our final model will accept categorical/numerical data on the MLP\n",
        "# input and images on the CNN input, outputting a single value (the\n",
        "# predicted price of the house)\n",
        "model = Model(inputs=[mlp.input, cnn.input], outputs=x)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0730 13:41:29.776602 140380935870336 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0730 13:41:29.817975 140380935870336 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0730 13:41:29.825686 140380935870336 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0730 13:41:29.914289 140380935870336 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "W0730 13:41:29.915540 140380935870336 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "W0730 13:41:30.183737 140380935870336 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "W0730 13:41:30.270931 140380935870336 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "W0730 13:41:30.575879 140380935870336 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIOdM1ohrzXa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "55f7aa91-d464-4135-9208-9e7fe924d68d"
      },
      "source": [
        "opt = Adam(lr=1e-3, decay=1e-3 / 200)\n",
        "model.compile(loss=\"mean_absolute_percentage_error\", optimizer=opt)\n",
        "model.summary()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0730 13:41:30.664004 140380935870336 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 64, 64, 3)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 64, 64, 16)   448         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 64, 64, 16)   0           conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 64, 64, 16)   64          activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 32, 32, 16)   0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 32, 32, 32)   4640        max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 32, 32, 32)   0           conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 32, 32, 32)   128         activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 16, 16, 32)   0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 16, 16, 64)   18496       max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 16, 16, 64)   0           conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 16, 16, 64)   256         activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2D)  (None, 8, 8, 64)     0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 4096)         0           max_pooling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 16)           65552       flatten_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 16)           0           dense_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 16)           64          activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dense_1_input (InputLayer)      (None, 10)           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 16)           0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 8)            88          dense_1_input[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 4)            68          dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 4)            36          dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 4)            0           dense_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 8)            0           dense_2[0][0]                    \n",
            "                                                                 activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 4)            36          concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 1)            5           dense_5[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 89,881\n",
            "Trainable params: 89,625\n",
            "Non-trainable params: 256\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3ZrvwMhtX_1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f5148d85-7107-4094-d638-bbcbad3825b8"
      },
      "source": [
        "model.fit([trainAttrX, trainImagesX], trainY, \n",
        "          validation_data=([testAttrX, testImagesX], testY),\t\n",
        "          epochs=200, \n",
        "          batch_size=8)\n",
        "\n",
        "model.save(\"housing_prices_stats_and_images_model.h5\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 271 samples, validate on 91 samples\n",
            "Epoch 1/200\n",
            "271/271 [==============================] - 4s 15ms/step - loss: 556.2186 - val_loss: 1337.9927\n",
            "Epoch 2/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 421.8579 - val_loss: 802.7460\n",
            "Epoch 3/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 277.8333 - val_loss: 408.6861\n",
            "Epoch 4/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 186.4672 - val_loss: 130.0478\n",
            "Epoch 5/200\n",
            "271/271 [==============================] - 3s 9ms/step - loss: 126.8539 - val_loss: 99.3998\n",
            "Epoch 6/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 102.3180 - val_loss: 76.5035\n",
            "Epoch 7/200\n",
            "271/271 [==============================] - 3s 9ms/step - loss: 111.1502 - val_loss: 72.6740\n",
            "Epoch 8/200\n",
            "271/271 [==============================] - 3s 9ms/step - loss: 80.0381 - val_loss: 65.4107\n",
            "Epoch 9/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 91.2461 - val_loss: 66.8556\n",
            "Epoch 10/200\n",
            "271/271 [==============================] - 3s 9ms/step - loss: 75.7193 - val_loss: 62.8345\n",
            "Epoch 11/200\n",
            "271/271 [==============================] - 3s 9ms/step - loss: 81.3108 - val_loss: 77.3097\n",
            "Epoch 12/200\n",
            "271/271 [==============================] - 3s 9ms/step - loss: 67.3420 - val_loss: 73.7509\n",
            "Epoch 13/200\n",
            "271/271 [==============================] - 3s 9ms/step - loss: 72.1636 - val_loss: 63.1875\n",
            "Epoch 14/200\n",
            "271/271 [==============================] - 3s 9ms/step - loss: 61.3058 - val_loss: 65.6610\n",
            "Epoch 15/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 62.0922 - val_loss: 62.2057\n",
            "Epoch 16/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 60.3842 - val_loss: 66.2628\n",
            "Epoch 17/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 56.9823 - val_loss: 58.2693\n",
            "Epoch 18/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 63.6049 - val_loss: 56.4869\n",
            "Epoch 19/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 63.4115 - val_loss: 64.0202\n",
            "Epoch 20/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 62.7303 - val_loss: 62.1649\n",
            "Epoch 21/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 56.9185 - val_loss: 62.0275\n",
            "Epoch 22/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 61.3748 - val_loss: 55.9945\n",
            "Epoch 23/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 56.0074 - val_loss: 61.3377\n",
            "Epoch 24/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 54.9858 - val_loss: 57.2456\n",
            "Epoch 25/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 55.3948 - val_loss: 58.5331\n",
            "Epoch 26/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 54.0162 - val_loss: 57.0711\n",
            "Epoch 27/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 52.5584 - val_loss: 52.3889\n",
            "Epoch 28/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 53.5919 - val_loss: 50.3030\n",
            "Epoch 29/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 52.2072 - val_loss: 53.4886\n",
            "Epoch 30/200\n",
            "271/271 [==============================] - 3s 9ms/step - loss: 54.7132 - val_loss: 55.9274\n",
            "Epoch 31/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 51.7925 - val_loss: 51.6080\n",
            "Epoch 32/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 49.7520 - val_loss: 48.1507\n",
            "Epoch 33/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 49.7384 - val_loss: 46.3781\n",
            "Epoch 34/200\n",
            "271/271 [==============================] - 3s 9ms/step - loss: 57.4990 - val_loss: 46.0123\n",
            "Epoch 35/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 49.6390 - val_loss: 50.3810\n",
            "Epoch 36/200\n",
            "271/271 [==============================] - 3s 9ms/step - loss: 50.2455 - val_loss: 43.8229\n",
            "Epoch 37/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 49.3854 - val_loss: 42.1655\n",
            "Epoch 38/200\n",
            "271/271 [==============================] - 3s 9ms/step - loss: 46.1837 - val_loss: 45.3146\n",
            "Epoch 39/200\n",
            "271/271 [==============================] - 3s 9ms/step - loss: 47.2107 - val_loss: 48.7532\n",
            "Epoch 40/200\n",
            "271/271 [==============================] - 3s 9ms/step - loss: 45.4200 - val_loss: 45.0978\n",
            "Epoch 41/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 43.9938 - val_loss: 42.5817\n",
            "Epoch 42/200\n",
            "271/271 [==============================] - 3s 9ms/step - loss: 44.9250 - val_loss: 45.7833\n",
            "Epoch 43/200\n",
            "271/271 [==============================] - 3s 9ms/step - loss: 43.3934 - val_loss: 44.8253\n",
            "Epoch 44/200\n",
            "271/271 [==============================] - 3s 9ms/step - loss: 42.3204 - val_loss: 46.3760\n",
            "Epoch 45/200\n",
            "271/271 [==============================] - 3s 9ms/step - loss: 42.9659 - val_loss: 44.1065\n",
            "Epoch 46/200\n",
            "271/271 [==============================] - 3s 9ms/step - loss: 42.0027 - val_loss: 43.3103\n",
            "Epoch 47/200\n",
            "271/271 [==============================] - 3s 9ms/step - loss: 42.2043 - val_loss: 55.2090\n",
            "Epoch 48/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 42.3631 - val_loss: 55.2051\n",
            "Epoch 49/200\n",
            "271/271 [==============================] - 3s 9ms/step - loss: 42.9697 - val_loss: 47.1647\n",
            "Epoch 50/200\n",
            "271/271 [==============================] - 3s 9ms/step - loss: 40.1301 - val_loss: 38.6962\n",
            "Epoch 51/200\n",
            "271/271 [==============================] - 3s 9ms/step - loss: 37.4233 - val_loss: 37.3732\n",
            "Epoch 52/200\n",
            "271/271 [==============================] - 3s 9ms/step - loss: 37.9163 - val_loss: 35.6543\n",
            "Epoch 53/200\n",
            "271/271 [==============================] - 3s 9ms/step - loss: 36.7151 - val_loss: 34.5158\n",
            "Epoch 54/200\n",
            "271/271 [==============================] - 3s 9ms/step - loss: 38.6278 - val_loss: 37.0696\n",
            "Epoch 55/200\n",
            "271/271 [==============================] - 3s 9ms/step - loss: 36.7655 - val_loss: 34.9942\n",
            "Epoch 56/200\n",
            "271/271 [==============================] - 3s 9ms/step - loss: 36.1277 - val_loss: 35.9074\n",
            "Epoch 57/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 34.3133 - val_loss: 32.5407\n",
            "Epoch 58/200\n",
            "271/271 [==============================] - 3s 9ms/step - loss: 32.7523 - val_loss: 32.5750\n",
            "Epoch 59/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 34.0725 - val_loss: 32.8400\n",
            "Epoch 60/200\n",
            "271/271 [==============================] - 3s 9ms/step - loss: 34.1409 - val_loss: 35.5658\n",
            "Epoch 61/200\n",
            "271/271 [==============================] - 3s 9ms/step - loss: 33.8845 - val_loss: 36.1084\n",
            "Epoch 62/200\n",
            "271/271 [==============================] - 3s 9ms/step - loss: 35.8114 - val_loss: 35.7935\n",
            "Epoch 63/200\n",
            "271/271 [==============================] - 3s 9ms/step - loss: 34.2724 - val_loss: 36.4217\n",
            "Epoch 64/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 32.4013 - val_loss: 32.0006\n",
            "Epoch 65/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 34.1362 - val_loss: 33.7502\n",
            "Epoch 66/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 33.2346 - val_loss: 33.9264\n",
            "Epoch 67/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 31.6662 - val_loss: 32.2192\n",
            "Epoch 68/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 33.8850 - val_loss: 36.9483\n",
            "Epoch 69/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 31.1012 - val_loss: 40.5484\n",
            "Epoch 70/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 30.1016 - val_loss: 37.5002\n",
            "Epoch 71/200\n",
            "271/271 [==============================] - 3s 9ms/step - loss: 30.1891 - val_loss: 36.8829\n",
            "Epoch 72/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 29.3270 - val_loss: 35.3008\n",
            "Epoch 73/200\n",
            "271/271 [==============================] - 3s 9ms/step - loss: 33.1354 - val_loss: 31.8547\n",
            "Epoch 74/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 29.2928 - val_loss: 31.3801\n",
            "Epoch 75/200\n",
            "271/271 [==============================] - 3s 9ms/step - loss: 31.0696 - val_loss: 35.8786\n",
            "Epoch 76/200\n",
            "271/271 [==============================] - 3s 9ms/step - loss: 29.2648 - val_loss: 32.1332\n",
            "Epoch 77/200\n",
            "271/271 [==============================] - 3s 9ms/step - loss: 31.0884 - val_loss: 31.1949\n",
            "Epoch 78/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 30.7714 - val_loss: 32.4781\n",
            "Epoch 79/200\n",
            "271/271 [==============================] - 3s 9ms/step - loss: 30.4630 - val_loss: 28.1149\n",
            "Epoch 80/200\n",
            "271/271 [==============================] - 3s 9ms/step - loss: 28.5235 - val_loss: 25.4057\n",
            "Epoch 81/200\n",
            "271/271 [==============================] - 3s 9ms/step - loss: 29.3343 - val_loss: 27.7916\n",
            "Epoch 82/200\n",
            "271/271 [==============================] - 3s 9ms/step - loss: 30.9062 - val_loss: 31.3893\n",
            "Epoch 83/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 30.9069 - val_loss: 32.8046\n",
            "Epoch 84/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 30.3633 - val_loss: 34.8155\n",
            "Epoch 85/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 28.6484 - val_loss: 30.5397\n",
            "Epoch 86/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 29.3814 - val_loss: 34.6667\n",
            "Epoch 87/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 29.5779 - val_loss: 32.6457\n",
            "Epoch 88/200\n",
            "271/271 [==============================] - 3s 9ms/step - loss: 28.1699 - val_loss: 30.5122\n",
            "Epoch 89/200\n",
            "271/271 [==============================] - 3s 9ms/step - loss: 24.7323 - val_loss: 32.4131\n",
            "Epoch 90/200\n",
            "271/271 [==============================] - 3s 9ms/step - loss: 26.5358 - val_loss: 31.0758\n",
            "Epoch 91/200\n",
            "271/271 [==============================] - 3s 10ms/step - loss: 26.5579 - val_loss: 32.4638\n",
            "Epoch 92/200\n",
            "271/271 [==============================] - 3s 9ms/step - loss: 28.9176 - val_loss: 29.1187\n",
            "Epoch 93/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 26.7251 - val_loss: 28.5822\n",
            "Epoch 94/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 27.3828 - val_loss: 26.8069\n",
            "Epoch 95/200\n",
            "271/271 [==============================] - 3s 9ms/step - loss: 25.5688 - val_loss: 25.9144\n",
            "Epoch 96/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 31.7213 - val_loss: 42.1207\n",
            "Epoch 97/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 30.5451 - val_loss: 40.5014\n",
            "Epoch 98/200\n",
            "271/271 [==============================] - 3s 9ms/step - loss: 38.6707 - val_loss: 54.9305\n",
            "Epoch 99/200\n",
            "271/271 [==============================] - 3s 9ms/step - loss: 33.6656 - val_loss: 39.8727\n",
            "Epoch 100/200\n",
            "271/271 [==============================] - 3s 9ms/step - loss: 41.5458 - val_loss: 52.1859\n",
            "Epoch 101/200\n",
            "271/271 [==============================] - 3s 9ms/step - loss: 36.9594 - val_loss: 55.7051\n",
            "Epoch 102/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 38.1095 - val_loss: 29.9312\n",
            "Epoch 103/200\n",
            "271/271 [==============================] - 3s 9ms/step - loss: 36.1159 - val_loss: 89.3011\n",
            "Epoch 104/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 37.4326 - val_loss: 32.1160\n",
            "Epoch 105/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 41.4639 - val_loss: 29.5809\n",
            "Epoch 106/200\n",
            "271/271 [==============================] - 3s 9ms/step - loss: 41.9501 - val_loss: 43.4854\n",
            "Epoch 107/200\n",
            "271/271 [==============================] - 3s 9ms/step - loss: 41.5050 - val_loss: 34.1796\n",
            "Epoch 108/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 41.7567 - val_loss: 26.3321\n",
            "Epoch 109/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 35.8467 - val_loss: 37.3184\n",
            "Epoch 110/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 37.5058 - val_loss: 28.5525\n",
            "Epoch 111/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 33.6908 - val_loss: 29.3314\n",
            "Epoch 112/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 32.0872 - val_loss: 25.1498\n",
            "Epoch 113/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 29.2955 - val_loss: 22.1179\n",
            "Epoch 114/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 33.6252 - val_loss: 21.0421\n",
            "Epoch 115/200\n",
            "271/271 [==============================] - 3s 9ms/step - loss: 26.1673 - val_loss: 26.3565\n",
            "Epoch 116/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 26.6155 - val_loss: 28.3413\n",
            "Epoch 117/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 26.2600 - val_loss: 22.7100\n",
            "Epoch 118/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 29.0699 - val_loss: 26.3445\n",
            "Epoch 119/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 27.8901 - val_loss: 27.5990\n",
            "Epoch 120/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 26.1143 - val_loss: 22.0910\n",
            "Epoch 121/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 25.6703 - val_loss: 22.1597\n",
            "Epoch 122/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 24.0192 - val_loss: 22.2383\n",
            "Epoch 123/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 24.2471 - val_loss: 20.4724\n",
            "Epoch 124/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 24.2446 - val_loss: 26.1892\n",
            "Epoch 125/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 24.0073 - val_loss: 23.7306\n",
            "Epoch 126/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 23.1754 - val_loss: 25.3286\n",
            "Epoch 127/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 24.4324 - val_loss: 25.6700\n",
            "Epoch 128/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 28.6464 - val_loss: 21.4647\n",
            "Epoch 129/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 25.2108 - val_loss: 25.3973\n",
            "Epoch 130/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 23.5937 - val_loss: 23.8646\n",
            "Epoch 131/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 22.3931 - val_loss: 22.1061\n",
            "Epoch 132/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 24.1053 - val_loss: 26.7285\n",
            "Epoch 133/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 24.4311 - val_loss: 22.9896\n",
            "Epoch 134/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 24.1935 - val_loss: 25.1388\n",
            "Epoch 135/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 26.1713 - val_loss: 30.0678\n",
            "Epoch 136/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 32.0645 - val_loss: 32.3821\n",
            "Epoch 137/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 24.3188 - val_loss: 25.0982\n",
            "Epoch 138/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 23.5159 - val_loss: 22.9533\n",
            "Epoch 139/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 26.4893 - val_loss: 22.4579\n",
            "Epoch 140/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 24.3168 - val_loss: 29.2839\n",
            "Epoch 141/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 24.1873 - val_loss: 26.1387\n",
            "Epoch 142/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 32.5122 - val_loss: 21.1930\n",
            "Epoch 143/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 24.0896 - val_loss: 22.6000\n",
            "Epoch 144/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 23.1473 - val_loss: 23.7344\n",
            "Epoch 145/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 22.6603 - val_loss: 23.4007\n",
            "Epoch 146/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 22.4228 - val_loss: 22.2361\n",
            "Epoch 147/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 22.2534 - val_loss: 22.1599\n",
            "Epoch 148/200\n",
            "271/271 [==============================] - 3s 9ms/step - loss: 24.0900 - val_loss: 22.6193\n",
            "Epoch 149/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 22.3930 - val_loss: 23.4412\n",
            "Epoch 150/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 22.5946 - val_loss: 23.6473\n",
            "Epoch 151/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 20.4853 - val_loss: 22.8871\n",
            "Epoch 152/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 23.3896 - val_loss: 27.2487\n",
            "Epoch 153/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 23.9859 - val_loss: 21.8393\n",
            "Epoch 154/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 24.2475 - val_loss: 27.1574\n",
            "Epoch 155/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 22.9462 - val_loss: 22.3963\n",
            "Epoch 156/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 22.8965 - val_loss: 27.0982\n",
            "Epoch 157/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 24.3054 - val_loss: 21.6206\n",
            "Epoch 158/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 22.5045 - val_loss: 22.3321\n",
            "Epoch 159/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 21.2054 - val_loss: 21.7253\n",
            "Epoch 160/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 23.8695 - val_loss: 22.1920\n",
            "Epoch 161/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 24.8702 - val_loss: 29.1119\n",
            "Epoch 162/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 23.3428 - val_loss: 23.7966\n",
            "Epoch 163/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 21.7757 - val_loss: 23.0620\n",
            "Epoch 164/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 20.2915 - val_loss: 22.7283\n",
            "Epoch 165/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 23.2871 - val_loss: 22.0975\n",
            "Epoch 166/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 21.7903 - val_loss: 21.9568\n",
            "Epoch 167/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 21.8573 - val_loss: 22.5829\n",
            "Epoch 168/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 20.0096 - val_loss: 23.0429\n",
            "Epoch 169/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 23.1633 - val_loss: 22.7013\n",
            "Epoch 170/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 21.4873 - val_loss: 23.4846\n",
            "Epoch 171/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 20.3074 - val_loss: 22.9869\n",
            "Epoch 172/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 22.4576 - val_loss: 22.1147\n",
            "Epoch 173/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 21.2981 - val_loss: 22.3966\n",
            "Epoch 174/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 20.9988 - val_loss: 25.4083\n",
            "Epoch 175/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 20.5505 - val_loss: 22.0783\n",
            "Epoch 176/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 21.2132 - val_loss: 29.4892\n",
            "Epoch 177/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 22.3611 - val_loss: 22.5593\n",
            "Epoch 178/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 23.3707 - val_loss: 26.0292\n",
            "Epoch 179/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 22.3569 - val_loss: 21.0975\n",
            "Epoch 180/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 21.1535 - val_loss: 21.8129\n",
            "Epoch 181/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 20.2535 - val_loss: 21.8424\n",
            "Epoch 182/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 22.0075 - val_loss: 21.8606\n",
            "Epoch 183/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 20.2958 - val_loss: 22.6768\n",
            "Epoch 184/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 20.1251 - val_loss: 21.9871\n",
            "Epoch 185/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 19.8566 - val_loss: 22.8192\n",
            "Epoch 186/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 21.7392 - val_loss: 26.0985\n",
            "Epoch 187/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 23.5302 - val_loss: 26.1884\n",
            "Epoch 188/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 31.6411 - val_loss: 33.6569\n",
            "Epoch 189/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 28.0935 - val_loss: 24.3648\n",
            "Epoch 190/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 23.6680 - val_loss: 24.1991\n",
            "Epoch 191/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 21.6475 - val_loss: 22.5115\n",
            "Epoch 192/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 20.8811 - val_loss: 24.4604\n",
            "Epoch 193/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 20.9859 - val_loss: 24.1665\n",
            "Epoch 194/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 20.7725 - val_loss: 21.7561\n",
            "Epoch 195/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 19.7457 - val_loss: 22.2215\n",
            "Epoch 196/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 20.9480 - val_loss: 22.5542\n",
            "Epoch 197/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 20.0856 - val_loss: 22.1426\n",
            "Epoch 198/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 22.6468 - val_loss: 24.5386\n",
            "Epoch 199/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 21.0672 - val_loss: 21.4684\n",
            "Epoch 200/200\n",
            "271/271 [==============================] - 2s 9ms/step - loss: 21.2601 - val_loss: 22.1654\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACrj6YdGunef",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preds = model.predict([testAttrX, testImagesX])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybOE7rxGPK7_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "d084fce0-406e-4118-d0bc-05b4644bb083"
      },
      "source": [
        "import locale\n",
        "\n",
        "diff = preds.flatten() - testY\n",
        "percentDiff = (diff / testY) * 100\n",
        "absPercentDiff = np.abs(percentDiff)\n",
        " \n",
        "# compute the mean and standard deviation of the absolute percentage\n",
        "# difference\n",
        "mean = np.mean(absPercentDiff)\n",
        "std = np.std(absPercentDiff)\n",
        " \n",
        "# finally, show some statistics on our model\n",
        "locale.setlocale(locale.LC_ALL, \"en_US.UTF-8\")\n",
        "print(\"avg. house price: {}, std house price: {}\".format(\n",
        "\tlocale.currency(df[\"price\"].mean(), grouping=True),\n",
        "\tlocale.currency(df[\"price\"].std(), grouping=True)))\n",
        "print(\"mean: {:.2f}%, std: {:.2f}%\".format(mean, std))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "avg. house price: $533,388.27, std house price: $493,403.08\n",
            "mean: 22.17%, std: 19.80%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QuuHymY0uIeU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download(\"housing_prices_stats_and_images_model.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}